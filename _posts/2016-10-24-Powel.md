# Learnings from a DevOps Hack fest with Powel

![test](./images/frontpage.png)

## Introduction
In this DevOps Hack fest, Microsoft teamed up with Powel to hack a brand -new
solution, **SmartWater**. We describe the process and the result here, including
the following DevOps elements:

-   Infrastructure as Code
-   Continuous integration / Continuous delivery
-   Test, Staging and production environments

Additionally, the team expressed a need for getting hands-on experience with:

-   Test automation
-   Branching strategies
-   Security modelling

## Devops practices implemented


During the Hackfest, we implemented the following practices:

-   Created a multi-team project with work flowing from one team to the next
-   Use of Slack as a collaboration tool between departments
-   Set up a MicroService approach to deliver software updates in a “fast ring”
    style
-   Defined acceptance criteria according to the ISO standards for software
    quality
-   Used the SDL and the Microsoft Threat Modelling tool to assess security
    risks early
-   Architected the code using a Domain-Driven Design principles
-   Selected a long-lived feature branching strategy, using Git for Source
    Control
-   Adhered to the SOLID principles of object oriented design
-   Developed code using a Test-Driven approach
-   Set up an automated build & Test pipeline for each component in the solution
    (Continuous Integration)
-   Set up an automated release pipeline for testing and QA (Continuous
    Deployment)
-   Did targeted A/B testing based on tenant preference (each tenant opts-in to
    join a “fast ring” deployment)
-   Expecting to monitor running applications using Application Insights
-   Able to provision an entirely new Resource Group and apply that in a new /
    other resource group

We worked long and hard during every day of the Hackfest. Everyone was
high-spirited and excited about the work, and once Friday afternoon came, we had
a working proof-of-concept tying in all of the Api components with the web
Application and output from the Machine Learning algorithm.

## Hackfest members


| Company   | Participant         | Role                         | Twitter                                         |
| -------   | -----------         | ----                         | -------                                         |
| Powel     | Atle Vaaland        | Project Lead                 | [AtleVaa](http://twitter.com/AtleVaa)           | 
|           | Frøydis Sjøvold     | Domain expert, water         |                                                 | 
|           | Magnus Sæther       | Concept Designer             |                                                 |
|           | Jarle Skahjem       | Developer & Security Champ   | [J_Skahjem](http://twitter.com/J_Skahjem)       | 
|           | Hans Ruyne Bergheim | Developer                    | [hansbergheim](http://twitter.com/hansbergheim) | 
|           | Henning Mæland      | Developer                    |                                                 |
| BEKK      | Eivind Sorteberg    | Developer, ML, IoT Ingestion |                                                 |
| Microsoft | Pedro Dias          | Technical Evangelist         | [digitaldias](http://twitter.com/digitaldias)   | 
|           | Olav Tollefsen      | Technical Evangelist         | [olavtoll](http://twitter.com/olavtoll)         | 



Customer P­rofile
-----------------

[Powel](http://www.powel.com/) spans Europe with a broad and sustainable customer base and a long history
as trusted supplier of software solutions for cities / municipalities, counties,
the energy industry, as well as the contracting sector. Ca USD 50M revenue and
400 employees. The company has offices across Norway as well as in Sweden,
Denmark, Switzerland, Chile, Turkey and Poland.

Focus of the Hackfest
---------------------

Powel is investing in a brand new, born-in-the-cloud solution for monitoring
water in municipalities (Smart Water for Smart Cities). Undetected /
underreported water leakage accounts for over 30% of water loss and is the
single biggest factor in lost water. Using smart meters deployed in some
municipalities (and other sources) Powel will monitor water through Azure’s IoT
hub and analyze usage data to detect and predict maintenance issues. Detecting
water leaks and dynamically controlling water pressure throughout zones in
cities are the end goals of the product.

![The team hacking](./images/teamHacking.jpg)

**Image:** *Deeply focused team* 


Problem Statement
=================

Powel used a mix of older and newer tools and methods for source control,
quality control, tracking work and communication and deployment. Products
include *GIT, Team City* and *Octopus Deploy*.

The current software team has a lead time of just over 61 days on average, whit
a %Q&A of 0.8% in their existing projects.

The organization as a whole is divided on traditional lines (design, operations,
development, QA) and a significant challenge was to get this team to keep true
to the CI – QA – PROD mindset; to start thinking about deploy in the beginning
of the project and not at the end. Unit-Testing existed, but was unstructured,
and up to each developer. They had an awareness about best practices, but poor
or missing guidelines of how to do it. Powel would like to consider release
branching and feature toggling as well.

The challenges (areas of improvement) the Powel team has been facing can be
summed up as follows:

-   **Transforming** to support rapid delivery of new features to customers
-   **Formalizing** test acceptance criteria
-   **Assessing** risk on each feature delivered and establish practices to
    document and counter them

## Value Stream Map


The preparation for the DevOps Hack fest involved assessing the current state of
a similar project’s software development lifecycle, looking for areas of
improvement and focusing on the actual hack. During these phases, we narrowed
down which areas to improve and what to deliver, and then proceeded to just do
it!

### Value Stream Mapping Team


The Value Stream Mapping process (VSM) included the following people

| Company   | Name                  | Role                               | Twitter                                       |
| -------   | ----                  | ----                               | -------                                       | 
| Powel     | Atle Vaaland          | Project Lead                       | [AtleVaa](http://twitter.com/AtleVaa)         | 
|           | Frøydis Sjøvold       | Domain expert, water               |  |
|           | Hand-Tore Hetland     | Stand-in for Atle Vaaland on day 1 |  |  
|           | Kevin Gjerstad        | (future) CTO of Powel              |  |
|           | Tore Vestues          | Head of R&D                        |  |
|           | Jarle Skahjem         | Developer / Ops                    | [J_Skahjem](http://twitter.com/J_Skahjem) | 
|           | Per Harald Kristensen | Operations Lead                    |  |
|           | Henning Mæland        | Developer                          |  |
| Microsoft | Pedro Dias            | Technical Evangelist               | [digitaldias](http://twitter.com/digitaldias) | 



### Current Value Stream


In our initial Value Stream Mapping session (VSM) the Hack fest team discussed
the overall process, from feature definition to design, application development
cycle, technologies used, and routines for handing off from one stage to the
next.

![Value Stream Map of current development process done by the team](./images/e1fa47e4dcf290a731a46199c448da26.jpg) 

*Image: Value Stream Map of current development process done by the team*

Because it is a completely new solution to be defined, we did not have an MVP
(Minimum Viable Product) yet defined, so we decided to focus on the team’s
existing development process and discussed ways to improve that.
 
 ![Value Stream Map, Current State](./images/vsm_current.png)

**Diagram:** *Value Stream Map, Current State*

#### Lead Time vs Processing Time


The current Value Stream has a total completion lead time of just over 61
business days. Compared to the total processing time, this yielded a 30.1%
efficiency rate, meaning lead time represents 70% of that time.

#### Percent Complete and Accurate

The “*Percent complete and accurate*” (or simply: *%Q&A*) measures how many
times a feature passes from one stage to the next without having to come back
for review, fix or other reason. The rolled value here indicates that few, if
any of the features being delivered today require rework at some point in the
flow (*rolled %C&A 0.8%*). This is not an exceptional result, many companies may
have as many as 100% of their flow requiring rework at some stage, but we will
be aiming to deliver a stronger number here in our future map.

### Future State Value Stream


Having spent most of the time discussing and learning about Powel’s current
software development process, we proceeded to design a “perfect” future state
Value Stream.

![Whiteboard image of future state](images/70a9acda431da505414957b2d0247c48.jpg)

Because *SmartWater* is a greenfield project, the decision to scrap the current
way of development in favor a complete redesign addresses the central points of
concern gave the following map:

![Future State Value Stream map](./images/vsm_future.png)

**Diagram:** *Future State Value Stream map*

## Architecture


The SmartWater solution has a MicroService architecture consisting of a web
portal that calls into various underlying APIs. The web portal, as well as each
of the underlying APIs are each defined as epics, and detailed in features within each. 
The idea being that future products will be able to call into these APIs.

![Overall solution architecture diagram](./images/architecture.png)

## Multiple Teams across departments


In the Hackfest, we introduced Visual Studio Team Services (VSTS) as our overall
Application Lifetime Management (ALM) tool, allowing each department to track
changes to the product and structure their work into an effective flow between
the teams. The teams defined in VSTS are:

-   Design Team Design of mocks and formulate acceptance criteria
-   Developer Team Development of the feature, Test Automation
-   Ops Team Deployment, Infrastructure and Load Tests
-   Analytics Team Working with Machine Learning and advanced analytics

## Team Workflows 


Each deliverable component in the solution is represented as an Epic in VSTS.
Epics live outside all of the teams in the SmartWater project. When the need for
a feature within a component arises, that feature is added on a base level, and
then immediately assigned to the design team. A feature flows from one team to
the next until it is put into general release.

### Epic Level workflow

The following workflow was defined for the components at epic level:

Each Epic contains the features that define it.

![](./images/epicflow.png)

#### Feature definition


The features titles are written in a style similar to [Behavior Driven
Development](https://en.wikipedia.org/wiki/Behavior-driven_development) (BDD) where [Dan North](https://dannorth.net/about/), considered by many as the father of BDD. 
The idea is that a feature title should reflect the final outcome of said feature, i.e: 

"**The SettingsApi delivers the user's setting for Fast Ring deployment**"

This ubiqutos style gives everyone a very clear understanding of the feature just by looking at the title. 
Once the feature is pushed into prodution, a simple query in VSTS delivers a list of features that are in production. This way
everyone sees what the released product now can do.

#### Feature creation
Features are created in a brainstorming session with marketing, designers,
developers and operations.

The brainstorming session happens at epic level. Once defined, the features that
require the design team to do initial work is “transferred” to the design team
for acceptance criteria definition and production of design mocks. If it is a new feature for 
an API, it bypasses the designers, and is submitted directly to the developer team.

### Design team workflow

The design team carries their features across the following workflow:

![](./images/designerFlow.png)

Each feature needs an interaction design before a graphical design can be
applied to it. Once these two design stages are done, the feature should be
complete enough for defining its acceptance criteria to answer the question:

-   “When is this feature regarded as complete”?

#### Acceptance Criteria definition

The acceptance criteria should include the ISO standards [IEC 9126 / IEC 25010:2011](https://en.wikipedia.org/wiki/ISO/IEC_9126) for reference, among them:

-   Functional suitability/completeness
-   Reliability
-   Usability
-   Compatibility
-   Performance Efficiency
-   Maintainability
-   Security (Done by the developer team)

>**NOTE**: During the Hackfest, we did **not** have time to detail these.

Once the design and acceptance criteria have been defined, the feature is placed
in “Review” status. The idea being that senior leadership reviews the work done
by designers and give them the final “go”. This will not be a very formal
process initially, but marks good practice.

### Developer Workflow
The following flow helps management understand how the features are progressing.

![Developer Workflow](./images/developerFlow.png)

#### Threat modelling
Before the team starts to develop a feature, they create a threat model so that
any security issues can be added to the final acceptance criteria list.

#### Planning session and User Story creation
Before developers start working on a feature, they have a planning session where
user-stories are appended to the feature. These user stories are then broken
into individual tasks that are easier to estimate. This is a built-in feature of
VSTS.

### Operations Workflow

The operations workflow is simply Next Active Resolved (or Dismissed), as most
of the tasks involved in operations from a feature development are short-lived.
Operations also have the responsibility of monitoring a feature in production
environments, but that kind of operations does not have a development approach
and will be discussed later in this document

### Analysis Workflow

Like Operations, the features that the analysis team develop are not as detailed
as the steps in design and development, and are kept to the simple form of Next
Active Resolved (alternatively Dismissed). A feature of an API may require work
done by the analysis team (such as adding an API to the Machine Learning model)
and thus, may be assigned to this team when required.

### Collaborating with Slack
------------------------

We decided to use the collaboration tool Slack<sup>tm</sup> as a tool to prepare for the
Hackfest execution. With this, Powel had a project-specific discussion team that
was divided into the following topics:

| Topic           | Purpose            |
| -----           | -------            | 
| General         | General discussion |
| DevOps          | For discussions around code and operations |
| Dataweek        | Discussions around what kinds of data to use in the solution, and how to get them |
| MachineLearning | Discussions around Azure Machine learning topics |
| VSTS            | Automated hook into VSTS for showing failed and passed builds |

These chat-rooms were, and still are, actively used by the project team members and Microsoft, as they provide an easy, informal way of discussing the project
in detail. It is particularly interesting to see how all the project stakeholders can see when builds pass/fail, a seemingly simple thing that brings
the stakeholders that much closer to the overall flow.

## Hosting model


Currently, the architecture is delivered as a set of individually hosted Azure
webApps, but is flexible enough to be built into an Azure Service fabric at a
later stage if cost becomes an issue. Each webApi can be thought of as a
component in a Service Fabric and deployed separately. Today, the team delivers
each webApi to a hosted webApp in Azure.

## Domain-Driven Design structure


We organized the code in a [Domain-Driven Design](https://en.wikipedia.org/wiki/Domain-driven_design) (DDD) architecture, focusing on
the entities in the SmartWater solution, and the objects that operate upon them.
Thus, each component is built up as a collection of class libraries that
represent the different layers of execution: presentation, business, data, as
well as the domain that is common to all. This made a clean *[separation of
concerns](https://en.wikipedia.org/wiki/Separation_of_concerns)* and organized the projects cleanly, allowing different developers to
do work in separate levels of the same feature.

![SettingsApi organized in a DDD structure](images/d97c35121cf570a3449c70677f36d25a.png)

  
**Image:** *SettingsApi organized in a DDD structure*

### SOLID Principles of object oriented design

In order to provide a satisfying degree of test automation, we adopted the [SOLID principles of object oriented design](https://en.wikipedia.org/wiki/SOLID_(object-oriented_design)). 
This was mainly done to produce code that can be easily tested in isolation.

This document doesn’t go into the details of the SOLID principles, as these are
regarded as the very fundamental principles for doing test driven development.
The SOLID principles that we stressed over and over again during the Hackfest
were:

-   [Single Responsibility Principle](https://en.wikipedia.org/wiki/Single_responsibility_principle) : Any class should have only ONE reason to be changed. Keep it simple!
-   [Open/Closed Principle](https://en.wikipedia.org/wiki/Open/closed_principle): Any interface declaration is OPEN to expansion, but CLOSED to changes
-   [Dependency Inversion Principle](https://en.wikipedia.org/wiki/Dependency_inversion_principle): Essencially: the use of “new()” is
    banned. All depencencies are resolved using an IoC Container. Otherwise,
    Test-driven approaches are impossible.

### Source Control

We selected a “long-lived feature branch” source control strategy. Each product
in the architecture is developed and maintained in a separate souce control
branch.

Each branches triggers its own build/release automation cycle into a CI runtime
environment where the product is immediately available for testing. Once a
feature is deemed OK for general release, it is merged onto the master branch,
but the branch itself lives on for further development.

![Conceptual overview of the branches in the solution](./images/branches.png)

**Image**: *Conceptual overview of the branches in the solution*

>**NOTE:**
>
>This approach has issues; Merging common code with each of the API’s basically
>merges ALL the code from the master branch. We’re still searching for the
>optimal solution here, and have found one suggestion so far:
>
>Move all common code into versioned NuGet Library, and have each of the current
>branches become their own repositories.
>
>This is a trivial change to the existing application structure, and is probably
>already implemented by the time this document is realesed.

### Writing better commit messages

We adopted a strategy for writing commit messages that aims to completes the
following sentence with each commit:

*If this commit is applied, it will….***\<message\>**

This makes it easy to see the value-increase of each commit. The commit message
never explains *what* has been delivered, but rather what the change represents
to the product, and why it was necessary

Sample commit messages from an API branch:

```git
8b8d955 Add a leakage zone polygon mapper
be9d70f Merge branch 'master'
f39ab34 Finish test coverage for ExceptionHandler
910e3c3 Fix errors in spatial polygon and dates in documentDb queries
c4b445b Limit the query to docDb to 100 items for performance
e49a077 Update the API to wrap the results in DataResult
a1c1877 Rename IsValid() to Validate()
caba32d Add tenantVerifier and dateVerifier
```

## Test Automation
We built the applications using a TDD approach, writing Unit-Tests for each bit
of production code. This was done to solidify the product and give an early
alert on breaking changes.

The unit tests are written in the style of [Roy Osherove](http://osherove.com/about/) as suggested in his book [The Art of Unit Testing](https://www.amazon.com/Art-Unit-Testing-examples/dp/1617290890). 

A base-class for unit testing make dependencies of the class under tested automatically mocked, so that the
hassle of unit-tests goes away. The base class uses the nuGet package [StructureMap.AutoMocking.Moq](https://www.nuget.org/packages/structuremap.automocking.moq) to achieve this:

````csharp
public class ExceptionHandlerTests : TestsFor<ExceptionHandler>
{
    [Fact]
    public void Get_FunctionIsNull_DoesNotInvokeLogger()
    {
        // Arrange            
        Func<int> nullFunction = null;

        // Act           
        Instance.Get(nullFunction);

        // Assert
        GetMockFor<ILogger>().Verify(o => o.LogExceptionAsync(It.IsAny<Exception>(), It.IsAny<string>()), Times.Never());
    }
}
````
**Code sample:** *Test automation ensures the logic of the application is always verified during builds* 

Another nice NuGet package named [Should](https://www.nuget.org/packages/Should/) gave us a natural-language form of writing assertions that 
everyone instantly adopted:

````csharp
    [Fact]
    public async Task GetAsync_ValidFunction_ReturnsExpectedResult()
    {
        // Arrange
        Func<Task<int>> simpleFunction = () => Task.FromResult<int>(1313);

        // Act           
        var results = await Instance.GetAsync(simpleFunction);

        // Assert
        results.ShouldEqual(1313);
    }
````
**Code sample:** *Ùsing ***Should*** makes the assertions more readable* 

### Why test driven development matters
Developing code using TDD gave us the benefit of not needing to start up a full-running system in order to
try out a local change. Short Unit-Test will immediately confirm a theory, and uncover breaking changes in other areas of the code.

![Test-driven approach gives early warnings of breaking changes and help see which libraries are covered by tests.](images/3003663e3c2a86a4fd6031e615f6cec3.png)  

**Image:** *Test-driven approach gives early warnings of breaking changes and
help see which libraries are covered by tests.*


Even though code-coverage doesn’t uncover any quality issues, it was used in the
SmartWater project as an indicator of potentially problematic code. Libaries dealing with IO have a very limited set of
Unit-tests. 

>**NOTE:** Load testing will be implemented once the team has sorted out an AD
tenant to use in the CI-environment.


## Fast Ring/ Production Deployment
We decided to give the tenants the option to receive updates to the program in a
similar fashion to that of Windows 10. By providing the customer with a “Fast”
update option, they will, through their own configuration, be able to work on a
newer, “unreleased” version of the product. This gives us the ability to gather
feedback and metrics on that feature before deciding to release generally.

This was achieved by a simple configuration approach:

Each API is versioned. For example, the Api for retrieving infrastructure data
has the following address:

**https\:\<baseUrl\>v1/LeakageZones/\{tenantId\}**

A new version of the same API containing a change would then appear as:

**https\:\<baseUrl\>v2/LeakageZones/\{tenantId\}**

The API clients are configured with a value for *BaseUrl* ,
*ApiVersion* and *FastVersion*, and based upon the preference of the tenant,
they then read out the *FastVersion* to use the newest feature, or *ApiVersion*
if they haven’t opted in for early releases.

Once a feature is ready for general release, the *ApiVersion* and *FastVersion*
both point to the same values.

>**Note:** <br />
>We discussed, but did not land on any conclusion about how far back versions are to be maintained<br />


## Monitoring in production with Application Insights


This point was spoken of, but we didn’t have time to address it during the
Hackfest.

We are planning a Skype Session to go through Application Insights, and deliver
on the promise of getting vital production data from the system in production.

The general idea is to use Application Insights to track performance, custom
events and to detect unforeseen problems.

## Testing the product

Another follow-up is the use of Test Management capabilities in VSTS in order to
plan and execute tests.

The acceptance criteria made by the design team will be created as test plans in
VSTS that are executed once the feature has been delivered. These test plans
attach easily to the feature description in VSTS.

The test plans cover both functional tests as well as non-functional tests, such
as load testing and usability testing.

## Conclusion


After the Hackfest, Powel is now able to plan, design, develop and a release a
new feature within days. New features are estimated to have an average lead time
of 34 days or less, which a significant improvement to %C&A. This is basically
twice the current speed, and we are expecting the lead time to drop even further
as everyone starts getting into “the flow”.

With VSTS, project stakeholders can follow a feature as it goes through each of
the teams and into production, gaining insight into their development process
and adjusting as they go.

Adding acceptance criteria and considering security *before* a feature is
developed add up to a workflow with a much higer %C&A, as developers can see the
requirements and meet them at once.

The developers adopted modern styles of software development forming the basis
of a healthy codebase that is easy to maintain, and easy to understand for new
developers joining the team.
